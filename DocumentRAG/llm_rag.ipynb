{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a demo project to show the steps for creating a chat bot that can answer questions from documents. In this project we utilized a Llama-2–7b model, along with all-MiniLM-L6-v2an sentence-transformer embeddings model to create an AI chat bot that uses RAG(Retrieval-Augmented Generation). \n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional data. LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "DATA_PATH = 'demo_pdfs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(DATA_PATH, glob='*.pdf', loader_cls=PyPDFLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    "1. How the text is split\n",
    "2. How the chunk size is measured\n",
    "\n",
    "**Ref:** [Text Splitters\n",
    "](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='PSC-CPI: Multi-Scale Protein Sequence-Structure Contrasting for Efficient and\\nGeneralizable Compound-Protein Interaction Prediction\\nLirong Wu1,2, Yufei Huang1,2, Cheng Tan1,2, Zhangyang Gao1,2, Bozhen Hu1,2, Haitao Lin1,2,\\nZicheng Liu1,2, Stan Z. Li1,†\\n1AI Lab, Research Center for Industries of the Future, Westlake University, Hangzhou, China, 310030\\n2Zhejiang University, Hangzhou, China, 310058', metadata={'source': 'demo_pdfs\\\\PSC-CPI.pdf', 'page': 0})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ref:** [Hugging Face Embedding Models](https://python.langchain.com/docs/integrations/platforms/huggingface#embedding-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', \n",
    "                                   model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CTransformers(model='../models/llama-2-7b-chat.ggmlv3.q8_0.bin', \n",
    "                    model_type='llama', \n",
    "                    config={'max_new_tokens': 256,'temperature':0.3, 'context_length': 700}\n",
    "                    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful and accurate answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = prompt_template, \n",
    "                        input_variables=['context', 'question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Document's as output.\n",
    "\n",
    "**Ref:** [Retrievers](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={'k': 2}, \n",
    "                            return_source_documents=True, \n",
    "                            chain_type_kwargs={'prompt': prompt}\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff',retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"List the Deep learning-based Methods mentioned in the paper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'List the Deep learning-based Methods mentioned in the paper', 'result': ' The following deep learning-based methods are mentioned in the paper:\\n1. DeepDTA (Ozturk, Ozgur, and Ozkirimli, 2018)\\n2. Deep-ConvDTI (Lee, Keum, and Nam, 2019)\\n3. GraphDTA (Nguyen et al., 2021)\\n4. GNNs (Kipf and Welling, 2016; Wu et al., 2021a, 2022b, 2023)\\n5. RNNs (Armenteros et al., 2020)\\n6. TransformerCPI (Chen et al., 2020a)\\n7. HyperattentionDTI (Zhao et al., 2022)'}\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The following deep learning-based methods are mentioned in the paper:\n",
      "1. DeepDTA (Ozturk, Ozgur, and Ozkirimli, 2018)\n",
      "2. Deep-ConvDTI (Lee, Keum, and Nam, 2019)\n",
      "3. GraphDTA (Nguyen et al., 2021)\n",
      "4. GNNs (Kipf and Welling, 2016; Wu et al., 2021a, 2022b, 2023)\n",
      "5. RNNs (Armenteros et al., 2020)\n",
      "6. TransformerCPI (Chen et al., 2020a)\n",
      "7. HyperattentionDTI (Zhao et al., 2022)\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
